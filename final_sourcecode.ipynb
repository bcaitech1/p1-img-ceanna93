{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[PSTAGE] Image Classification.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNSmtXLYF14EkDxJHjBxUkL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9fr0duoskD1F"},"source":["# Image Classification\n","사람 이미지에서 마스크 착용 여부/성별/나이를 판별한다.\n","\n","||마스크|성별|나이|\n","|---:|:---:|:---:|:---:|\n","|0| Wear | Male | < 30 |\n","|1| Incorrect | Female | >= 30 and < 60  |\n","|2| Not wear | - | >= 60 |"]},{"cell_type":"code","metadata":{"id":"HVPK4GYvjgv7"},"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from glob import glob\n","import PIL\n","import torchvision\n","from torchvision import transforms\n","import torch.utils.data as data\n","from PIL import Image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import StratifiedKFold\n","import random\n","\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from albumentations import *\n","from albumentations.pytorch import ToTensorV2\n","\n","import os\n","\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR\n","from efficientnet_pytorch import EfficientNet\n","\n","import wandb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEu99EZSc6bb"},"source":["초기 설정"]},{"cell_type":"code","metadata":{"id":"K2iJThcVjqPx"},"source":["def seed_everything(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    \n","    print(f'이 실험은 seed {seed}로 고정되었습니다.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EJvtUk8jr-0"},"source":["class conf:\n","    seed = 2021\n","    data_dir = 'input/data/train'\n","    n_fold = 5\n","    batch_size = 8\n","    mask_class = 3\n","    gender_class = 2\n","    age_class = 3\n","    \n","seed_everything(conf.seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNtYTGdeZfF_"},"source":["data_dir = 'input/data/train'\n","img_dir = f'{data_dir}/images'\n","df_path = f'{data_dir}/train.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHirWXBLj5PI"},"source":["## Dataset\n","- 마스크 데이터셋\n","- 성별 데이터셋\n","- 나이 데이터셋\n","\n","분류하여 생성"]},{"cell_type":"code","metadata":{"id":"hj3u4mDzZeO7"},"source":["def get_mask_label(image_path):\n","    image_name = image_path.split('/')[-1]\n","    if 'incorrect' in image_name:\n","        return 1\n","    elif 'normal' in image_name:\n","        return 2\n","    elif 'mask' in image_name:\n","        return 0\n","    else:\n","        raise ValueError(f\"No mask class for {image_name}\")\n","        \n","def get_gender_label(image_path):\n","    image_name = image_path.split('/')[-1]\n","    profile = image_path.split('/')[-2]\n","    image_id, gender, race, age = profile.split(\"_\")\n","    if 'male' == gender:\n","        return 0\n","    elif 'female' == gender:\n","        return 1\n","    else:\n","        raise ValueError(f\"No gender class for {image_name}\")\n","\n","def get_age_label(image_path):\n","    image_name = image_path.split('/')[-1]\n","    profile = image_path.split('/')[-2]\n","    image_id, gender, race, age = profile.split(\"_\")\n","    return 0 if int(age) < 30 else 1 if int(age) < 60 else 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jbjq4dIHj1m9"},"source":["IMG_EXTENSIONS = [\n","    \".jpg\", \".JPG\", \".jpeg\", \".JPEG\", \".png\",\n","    \"PNG\", \".ppm\", \".PPM\", \".bmp\", \".BMP\"\n","]\n","\n","def is_image_file(filepath):\n","    return any(filepath.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","def remove_hidden_file(filepath):\n","    filename = filepath.split('/')[-1]\n","    return False if filename.startswith('._') else True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38X4ODa2UO7L"},"source":["def get_img(path):\n","    im_bgr = cv2.imread(path)\n","    im_rgb = im_bgr[:, :, ::-1]\n","    return im_rgb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_dExp0mESyhS"},"source":["#### MaskDataset"]},{"cell_type":"code","metadata":{"id":"eGd4VZnVkp5e"},"source":["class MaskDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        \n","        self.image_paths = []\n","        profiles = os.listdir(self.image_dir)\n","        for profile in profiles:\n","            if remove_hidden_file(profile):\n","                for file_name in os.listdir(f'{image_dir}/{profile}'):\n","                    img_path = os.path.join(image_dir, profile, file_name)\n","                    if is_image_file(img_path):\n","                        self.image_paths.append(img_path)\n","        \n","        self.image_paths = list(filter(is_image_file, self.image_paths))\n","        self.image_paths = list(filter(remove_hidden_file, self.image_paths))\n","        \n","        self.labels = [get_mask_label(path) for path in self.image_paths]\n","        \n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx];\n","        label = self.labels[idx]\n","        image = get_img(image_path)\n","        \n","        if self.transform:\n","            image = self.transform(image = image)['image']\n","        #label = torch.eye(18)[label]\n","        return image, label\n","    \n","    def __len__(self):\n","        return len(self.image_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pus52400S2vB"},"source":["#### GenderDataset"]},{"cell_type":"code","metadata":{"id":"nOmgQpweSuIg"},"source":["class GenderDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        \n","        self.image_paths = []\n","        profiles = os.listdir(self.image_dir)\n","        for profile in profiles:\n","            if remove_hidden_file(profile):\n","                for file_name in os.listdir(f'{image_dir}/{profile}'):\n","                    img_path = os.path.join(image_dir, profile, file_name)\n","                    if is_image_file(img_path):\n","                        self.image_paths.append(img_path)\n","        \n","        self.image_paths = list(filter(is_image_file, self.image_paths))\n","        self.image_paths = list(filter(remove_hidden_file, self.image_paths))\n","        \n","        self.labels = [get_gender_label(path) for path in self.image_paths]\n","        \n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx];\n","        label = self.labels[idx]\n","        image = get_img(image_path)\n","        \n","        if self.transform:\n","            image = self.transform(image = image)['image']\n","        #label = torch.eye(18)[label]\n","        return image, label\n","    \n","    def __len__(self):\n","        return len(self.image_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KchMZJVS4iV"},"source":["#### AgeDataset"]},{"cell_type":"code","metadata":{"id":"IF_MMZCASwwb"},"source":["class AgeDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        \n","        self.image_paths = []\n","        profiles = os.listdir(self.image_dir)\n","        for profile in profiles:\n","            if remove_hidden_file(profile):\n","                for file_name in os.listdir(f'{image_dir}/{profile}'):\n","                    img_path = os.path.join(image_dir, profile, file_name)\n","                    if is_image_file(img_path):\n","                        self.image_paths.append(img_path)\n","        self.image_paths = list(filter(is_image_file, self.image_paths))\n","        self.image_paths = list(filter(remove_hidden_file, self.image_paths))\n","        \n","        self.labels = [get_age_label(path) for path in self.image_paths]\n","        \n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx];\n","        label = self.labels[idx]\n","        image = get_img(image_path)\n","        \n","        if self.transform:\n","            image = self.transform(image = image)['image']\n","        #label = torch.eye(18)[label]\n","        return image, label\n","    \n","    def __len__(self):\n","        return len(self.image_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EpiM7qrjS-eo"},"source":["## Transforms"]},{"cell_type":"code","metadata":{"id":"DScOm8pOkqvl"},"source":["def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n","    transformations = {}\n","    if 'train' in need:\n","        transformations['train'] = Compose([\n","            CenterCrop(448, 336, p=1.0),\n","            Resize(img_size[0], img_size[1], p=1.0),\n","            HorizontalFlip(p=0.5),\n","            ShiftScaleRotate(p=0.3),\n","            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.3),\n","            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.3),\n","            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n","            CoarseDropout(p=0.3),\n","            GaussNoise(p=0.3),\n","            Cutout(p=0.3),\n","            ToTensorV2(p=1.0),\n","        ], p=1.0)\n","    if 'val' in need:\n","        transformations['val'] = Compose([\n","            CenterCrop(448, 336, p=1.0),\n","            Resize(img_size[0], img_size[1], p=1.0),\n","            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n","            ToTensorV2(p=1.0),\n","        ], p=1.0)\n","    return transformations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--peXeBqilVJ"},"source":["custom_transforms = get_transforms()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnITaA4Jj2WM"},"source":["mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9TMsYQGyN4t"},"source":["## 모델\n","- Mask/Gender/Age 모델을 생성\n","  - pretrained된 efficientnet-b4\n","- Loss 함수 설정\n","- Optimizer 설정"]},{"cell_type":"code","metadata":{"id":"SG4b0oNwyNIQ"},"source":["MaskModel = EfficientNet.from_pretrained(\"efficientnet-b4\", num_classes=conf.mask_class)\n","GenderModel = EfficientNet.from_pretrained(\"efficientnet-b4\", num_classes=conf.gender_class)\n","AgeModel = EfficientNet.from_pretrained(\"efficientnet-b4\", num_classes=conf.age_class)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9g40n5yXzqdf"},"source":["criterion = CrossEntropyLoss().cuda()\n","mask_optimizer = AdamP(MaskModel.parameters(), lr=1e-4)\n","gender_optimizer = AdamP(GenderModel.parameters(), lr=1e-4)\n","age_optimizer = AdamP(AgeModel.parameters(), lr=1e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifTcxsfO0h4j"},"source":["MaskModel.cuda()\n","GenderModel.cuda()\n","AgeModel.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIWD8AkD_PLz"},"source":["## DataLoader\n","Imbalance한 데이터에 CrossEntropyLoss를 사용하기 위해 WeightedRandomSampler를 사용하기로 결정\n","\n","Counter를 이용해 각 라벨의 분포를 확인하고 weight 값을 계산"]},{"cell_type":"code","metadata":{"id":"5cOqgMjH_OjM"},"source":["from collections import Counter\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElUa7eCrASNa"},"source":["#### Mask Train dataset\n","Mask의 경우 WeightedRandomSampler 사용이 필요"]},{"cell_type":"code","metadata":{"id":"D7z_8KiC_HEj"},"source":["mask_train_dataset = MaskDataset(img_dir, custom_transforms['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2P7sPh6a_hVG"},"source":["mask_counter = Counter(mask_train_dataset.labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzTpWNOA-5v-"},"source":["plt.figure(figsize=(8,8))\n","sns.barplot(data = pd.DataFrame.from_dict([mask_counter]).melt(), x = \"variable\", y=\"value\", hue=\"variable\").set_title('Natural Images Class Distribution')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mX9ttkcA-6Z-"},"source":["mask_class_count = [i for i in mask_counter.values()]\n","mask_class_weights = 1./torch.tensor(mask_class_count, dtype=torch.float)\n","mask_class_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0tWJjHX4R9o"},"source":["mask_class_weights_all = mask_class_weights[mask_train_dataset.labels]\n","mask_class_weights_all"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6Fc_LbeaGbw"},"source":["mask_weighted_sampler = WeightedRandomSampler(\n","    weights=mask_class_weights_all,\n","    num_samples=len(mask_class_weights_all),\n","    replacement=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsUPDMOxaIlz"},"source":["mask_train_loader = DataLoader(mask_train_dataset, batch_size = conf.batch_size, shuffle=False, num_workers=3, sampler=mask_weighted_sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRqrFZ5a_psL"},"source":["#### Gender Train dataset\n","Gender의 경우 분포가 거의 비슷하기 때문에 WeightedRandomSampler가 필요하지 않음"]},{"cell_type":"code","metadata":{"id":"q2MBYlJE_vSA"},"source":["gender_train_dataset = GenderDataset(img_dir, custom_transforms['train'])\n","gender_train_loader = DataLoader(gender_train_dataset, batch_size = conf.batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YlfVbELiAaYj"},"source":["#### Age Train dataset\n","Age의 경우 WeightedRandomSampler 사용이 필요"]},{"cell_type":"code","metadata":{"id":"1dSY19S9AQxL"},"source":["age_train_dataset = AgeDataset(img_dir, custom_transforms['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdCx6lB4Ajp4"},"source":["age_counter = Counter(age_train_dataset.labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAXF6QlYaRPG"},"source":["plt.figure(figsize=(24,24))\n","sns.barplot(data = pd.DataFrame.from_dict([age_counter]).melt(), x = \"variable\", y=\"value\", hue=\"variable\").set_title('Natural Images Class Distribution')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sLec71PAlZx"},"source":["age_class_count = [i for i in age_counter.values()]\n","age_class_weights = 1./torch.tensor(age_class_count, dtype=torch.float)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6Rw0r8oAm0g"},"source":["age_class_weights_all = age_class_weights[age_train_dataset.labels]\n","age_class_weights_all"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uydUW8epaVFY"},"source":["age_weighted_sampler = WeightedRandomSampler(\n","    weights=age_class_weights_all,\n","    num_samples=len(age_class_weights_all),\n","    replacement=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zj9344o0aXJx"},"source":["age_train_loader = DataLoader(age_train_dataset, batch_size = conf.batch_size, shuffle=False, num_workers=3, sampler=age_weighted_sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wx-c7JOJCNMl"},"source":["## 학습"]},{"cell_type":"markdown","metadata":{"id":"Z8p0qQR537ei"},"source":["#### 학습을 위한 초기 설정"]},{"cell_type":"code","metadata":{"id":"skcm4SPvCTO6"},"source":["num_epochs = 5  # 학습할 epoch의 수\n","lr = 1e-4\n","lr_decay_step = 10\n","train_log_interval = 20  # logging할 iteration의 주기\n","\n","# train_log_interval = 20  # logging할 iteration의 주기\n","mask_name = \"mask_model\"  # 결과를 저장하는 폴더의 이름\n","gender_name = \"gender_model\"\n","age_name = \"age_model\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6ZEs5Lf35I7"},"source":["### MaskModel 학습"]},{"cell_type":"code","metadata":{"id":"kfHnfQ5pcfza"},"source":["mask_scheduler = StepLR(mask_optimizer, lr_decay_step, gamma=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AL-zOMbiCMhx"},"source":["os.makedirs(os.path.join(os.getcwd(), 'results', mask_name), exist_ok=True)\n","\n","counter = 0\n","patience = 10\n","accumulation_steps = 2\n","best_val_acc = 0\n","best_val_loss = np.inf\n","for epoch in range(num_epochs):\n","    # train loop\n","    MaskModel.train()\n","    loss_value = 0\n","    matches = 0\n","    for idx, train_batch in enumerate(mask_train_loader):\n","        inputs, labels = train_batch\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        outs = MaskModel(inputs)\n","        pred = outs.data.cpu().numpy()\n","        preds = torch.argmax(outs, dim=-1)\n","        loss = criterion(outs, labels)\n","\n","        loss.backward()\n","        \n","        # -- Gradient Accumulation\n","        if (idx+1) % accumulation_steps == 0:\n","            mask_optimizer.step()\n","            mask_optimizer.zero_grad()\n","\n","        loss_value += loss.item()\n","        matches += (preds == labels).sum().item()\n","        if (idx + 1) % train_log_interval == 0:\n","            train_loss = loss_value / train_log_interval\n","            train_acc = matches / conf.batch_size / train_log_interval\n","            current_lr = mask_scheduler.get_last_lr()\n","            print(\n","                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(mask_train_loader)}) || \"\n","                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n","            )\n","            \n","            if train_acc > best_val_acc:\n","                print(\"New best model for val accuracy! saving the model..\")\n","                torch.save(MaskModel.state_dict(), f\"results/{mask_name}/{epoch:03}_accuracy_{train_acc:4.2%}.ckpt\")\n","                best_val_acc = train_acc\n","                counter = 0\n","\n","            loss_value = 0\n","            matches = 0\n","\n","    mask_scheduler.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYhQzYAU322Z"},"source":["### GenderModel 학습"]},{"cell_type":"code","metadata":{"id":"yH9LWqQkclUa"},"source":["gender_scheduler = StepLR(gender_optimizer, lr_decay_step, gamma=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJpV-sDzcmwN"},"source":["os.makedirs(os.path.join(os.getcwd(), 'results', gender_name), exist_ok=True)\n","\n","counter = 0\n","patience = 10\n","accumulation_steps = 2\n","best_val_acc = 0\n","best_val_loss = np.inf\n","for epoch in range(num_epochs):\n","    # train loop\n","    GenderModel.train()\n","    loss_value = 0\n","    matches = 0\n","    for idx, train_batch in enumerate(gender_train_loader):\n","        inputs, labels = train_batch\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        outs = GenderModel(inputs)\n","        pred = outs.data.cpu().numpy()\n","        preds = torch.argmax(outs, dim=-1)\n","        loss = criterion(outs, labels)\n","\n","        loss.backward()\n","        \n","        # -- Gradient Accumulation\n","        if (idx+1) % accumulation_steps == 0:\n","            gender_optimizer.step()\n","            gender_optimizer.zero_grad()\n","\n","        loss_value += loss.item()\n","        matches += (preds == labels).sum().item()\n","        if (idx + 1) % train_log_interval == 0:\n","            train_loss = loss_value / train_log_interval\n","            train_acc = matches / conf.batch_size / train_log_interval\n","            current_lr = gender_scheduler.get_last_lr()\n","            print(\n","                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(gender_train_loader)}) || \"\n","                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n","            )\n","            \n","            if train_acc > best_val_acc:\n","                print(\"New best model for val accuracy! saving the model..\")\n","                torch.save(GenderModel.state_dict(), f\"results/{gender_name}/{epoch:03}_accuracy_{train_acc:4.2%}.ckpt\")\n","                best_val_acc = train_acc\n","                counter = 0\n","\n","            loss_value = 0\n","            matches = 0\n","\n","    gender_scheduler.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TOmgZKAS3mKO"},"source":["### AgeModel 학습"]},{"cell_type":"code","metadata":{"id":"tXcyKeqjcoil"},"source":["age_scheduler = StepLR(age_optimizer, lr_decay_step, gamma=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwoqmENKcox3"},"source":["os.makedirs(os.path.join(os.getcwd(), 'results', age_name), exist_ok=True)\n","\n","counter = 0\n","patience = 10\n","accumulation_steps = 2\n","best_val_acc = 0\n","best_val_loss = np.inf\n","for epoch in range(num_epochs):\n","    # train loop\n","    AgeModel.train()\n","    loss_value = 0\n","    matches = 0\n","    for idx, train_batch in enumerate(age_train_loader):\n","        inputs, labels = train_batch\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        outs = AgeModel(inputs)\n","        pred = outs.data.cpu().numpy()\n","        preds = torch.argmax(outs, dim=-1)\n","        \n","        loss = criterion(outs, labels)\n","\n","        loss.backward()\n","        \n","        # -- Gradient Accumulation\n","        if (idx+1) % accumulation_steps == 0:\n","            age_optimizer.step()\n","            age_optimizer.zero_grad()\n","\n","        loss_value += loss.item()\n","        matches += (preds == labels).sum().item()\n","        if (idx + 1) % train_log_interval == 0:\n","            train_loss = loss_value / train_log_interval\n","            train_acc = matches / conf.batch_size / train_log_interval\n","            current_lr = age_scheduler.get_last_lr()\n","            print(\n","                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(age_train_loader)}) || \"\n","                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n","            )\n","            \n","            if train_acc > best_val_acc:\n","                print(\"New best model for val accuracy! saving the model..\")\n","                torch.save(AgeModel.state_dict(), f\"results/{age_name}/{epoch:03}_accuracy_{train_acc:4.2%}.ckpt\")\n","                best_val_acc = train_acc\n","                counter = 0\n","\n","            loss_value = 0\n","            matches = 0\n","\n","    age_scheduler.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cT_hSW694TGa"},"source":["### Inference를 위한 모델\n","Image를 입력받아 위에 정의한 MaskModel, GenderModel, AgeModel를 통해 출력한 각 결과들을 하나의 라벨로 합쳐 반환"]},{"cell_type":"code","metadata":{"id":"hON-tDyAcrPO"},"source":["class MyEnsemble(nn.Module):\n","    def __init__(self, modelA, modelB, modelC, nb_classes=18):\n","        super(MyEnsemble, self).__init__()\n","        self.modelA = modelA\n","        self.modelB = modelB\n","        self.modelC = modelC\n","        \n","    def forward(self, x):\n","        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n","        x2 = self.modelB(x.clone())\n","        x3 = self.modelC(x.clone())\n","        \n","        return x1.argmax(dim=-1).item() * 6 + x2.argmax(dim=-1).item() * 3 + x3.argmax(dim=-1).item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkWUBSXgcusB"},"source":["ensemble = MyEnsemble(MaskModel, GenderModel, AgeModel)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yas5MWGi5EzL"},"source":["### Inference"]},{"cell_type":"code","metadata":{"id":"zPdhBuYOcvtA"},"source":["eval_dir = 'input/data/eval'\n","eval_img_dir = f'{eval_dir}/images'\n","eval_df_path = f'{eval_dir}/info.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DeFUEvZRcyNt"},"source":["class TestDataset(Dataset):\n","    def __init__(self, img_paths, transform):\n","        self.img_paths = img_paths\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        image_path = self.img_paths[index];\n","        image = get_img(image_path)\n","        \n","        if self.transform:\n","            image = self.transform(image = image)['image']\n","        return image\n","\n","    def __len__(self):\n","        return len(self.img_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdjcw1j0cyea"},"source":["import torchvision\n","\n","submission = pd.read_csv(os.path.join(eval_dir, 'info.csv'))\n","image_dir = os.path.join(eval_dir, 'images')\n","\n","# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n","image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n","\n","tf = torchvision.transforms.Compose([\n","    CenterCrop(448, 336, p=1.0),\n","    Resize(512, 384, p=1.0),\n","    Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n","    ToTensorV2(p=1.0),\n","])\n","\n","test_dataset = TestDataset(image_paths, custom_transforms['val'])\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    shuffle=False\n",")\n","\n","ensemble.eval()\n","\n","# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n","all_predictions = []\n","for images in test_loader:\n","    with torch.no_grad():\n","        images = images.cuda()\n","        pred = ensemble(images)\n","        all_predictions.append(pred)\n","submission['ans'] = all_predictions\n","\n","# 제출할 파일을 저장합니다.\n","submission.to_csv(os.path.join(eval_dir, 'submission_modelsplit.csv'), index=False)\n","print('test inference is done!')"],"execution_count":null,"outputs":[]}]}